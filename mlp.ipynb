{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ceeF6QrSOnj"
      },
      "source": [
        "# Preparação Dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UQhEHK3TSOnn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Leitura dos dados originais\n",
        "# Mostrar todas as colunas\n",
        "pd.set_option('display.max_columns', None)\n",
        "file_path = 'data/TUCURUI_DIARIA_1939_2016.xlsx'\n",
        "data = pd.read_excel(file_path, header=None, names=['Vazao'])\n",
        "\n",
        "# Função para calcular derivadas percentuais\n",
        "\n",
        "\n",
        "def calcular_derivadas(data):\n",
        "    data['V1'] = data['Vazao'].pct_change() * 100  # Primeira derivada\n",
        "    data['V2'] = data['V1'].diff()  # Segunda derivada\n",
        "    return data\n",
        "\n",
        "# Função para criar defasagens\n",
        "\n",
        "\n",
        "def criar_defasagens(data, colunas, entradas):\n",
        "    defasagens_list = []\n",
        "\n",
        "    for col in colunas:\n",
        "        for i in range(entradas, 0, -1):\n",
        "            defasagens_list.append(data[col].shift(\n",
        "                i).rename(f'Entrada_{col}_{i}'))\n",
        "        defasagens_list.append(data[col])\n",
        "\n",
        "    for i in range(1, 12):\n",
        "        defasagens_list.append(data['Vazao'].shift(-i).rename(f'Saida_{i}'))\n",
        "    defasagens = pd.concat(defasagens_list, axis=1)\n",
        "    return defasagens\n",
        "\n",
        "\n",
        "# Função para criar colunas binárias\n",
        "def criar_colunas_binarias(data):\n",
        "    # Inicializar as colunas binárias com zeros\n",
        "    data['Bin1'] = 0\n",
        "    data['Bin2'] = 0\n",
        "    data['Bin3'] = 0\n",
        "\n",
        "    # Aplicar as condições para definir os valores das colunas binárias\n",
        "    data.loc[(data['V1'] > 15) & (data['V2'] > 0),\n",
        "             ['Bin1', 'Bin2', 'Bin3']] = [1, 0, 0]\n",
        "    data.loc[(data['V1'] < -15) & (data['V2'] < 0),\n",
        "             ['Bin1', 'Bin2', 'Bin3']] = [0, 0, 1]\n",
        "    data.loc[~((data['V1'] > 15) & (data['V2'] > 0)) & ~(\n",
        "        (data['V1'] < -15) & (data['V2'] < 0)), ['Bin1', 'Bin2', 'Bin3']] = [0, 1, 0]\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsTGRdBbSOnp"
      },
      "source": [
        "# **MODELO 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_ANQS6ESOnq",
        "outputId": "455f3d3f-0447-4f7a-9df0-0fc455062921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrames salvos com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Criar datasets com defasagem apenas na coluna 'Vazao'\n",
        "data_15 = criar_defasagens(data.copy(), ['Vazao'], 15)\n",
        "data_30 = criar_defasagens(data.copy(), ['Vazao'], 30)\n",
        "data_45 = criar_defasagens(data.copy(), ['Vazao'], 45)\n",
        "\n",
        "# Pasta onde os arquivos serão salvos\n",
        "\n",
        "output_folder = 'data/modelo_1'\n",
        "\n",
        "# Criar a pasta se ela não existir\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Caminhos para os arquivos Excel\n",
        "file_path_15 = os.path.join(output_folder, 'data_15_dias.xlsx')\n",
        "file_path_30 = os.path.join(output_folder, 'data_30_dias.xlsx')\n",
        "file_path_45 = os.path.join(output_folder, 'data_45_dias.xlsx')\n",
        "\n",
        "# Salvar os DataFrames em arquivos Excel\n",
        "data_15.to_excel(file_path_15, index=False)\n",
        "data_30.to_excel(file_path_30, index=False)\n",
        "data_45.to_excel(file_path_45, index=False)\n",
        "\n",
        "print(\"DataFrames salvos com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS6nrdRVSOns"
      },
      "source": [
        "# **MODELO 2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2F3LQqZSOnt",
        "outputId": "370ab10c-54ba-48ec-d5df-8014cd0bd4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrames salvos com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Calcular derivadas\n",
        "data_com_derivadas = calcular_derivadas(data)\n",
        "\n",
        "# Criar datasets com defasagem nas colunas 'V2', 'V1' e 'Vazao'\n",
        "data_derivadas_15 = criar_defasagens(data_com_derivadas, ['V2', 'V1','Vazao'], 15)\n",
        "data_derivadas_30 = criar_defasagens(data_com_derivadas, ['V2', 'V1','Vazao'], 30)\n",
        "data_derivadas_45 = criar_defasagens(data_com_derivadas, ['V2', 'V1','Vazao'], 45)\n",
        "\n",
        "# Pasta onde os arquivos serão salvos, dentro de 'data'\n",
        "output_folder = 'data/modelo_2'\n",
        "\n",
        "# Criar a pasta 'modelo_2' dentro de 'data' se ela não existir\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Caminhos  para os arquivos Excel\n",
        "file_path_derivadas_15 = os.path.join(output_folder, 'data_derivadas_15_dias.xlsx')\n",
        "file_path_derivadas_30 = os.path.join(output_folder, 'data_derivadas_30_dias.xlsx')\n",
        "file_path_derivadas_45 = os.path.join(output_folder, 'data_derivadas_45_dias.xlsx')\n",
        "\n",
        "# Salvar os DataFrames em arquivos Excel\n",
        "data_derivadas_15.to_excel(file_path_derivadas_15, index=False)\n",
        "data_derivadas_30.to_excel(file_path_derivadas_30, index=False)\n",
        "data_derivadas_45.to_excel(file_path_derivadas_45, index=False)\n",
        "\n",
        "print(\"DataFrames salvos com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mLWpFnrfSQNu",
        "outputId": "7b201560-79cf-4f0c-e4c5-38ab775c1553"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V2</th>\n",
              "      <th>V1</th>\n",
              "      <th>Vazao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.995261</td>\n",
              "      <td>2089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.871661</td>\n",
              "      <td>-1.866922</td>\n",
              "      <td>2050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.084298</td>\n",
              "      <td>-1.951220</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.005946</td>\n",
              "      <td>-0.945274</td>\n",
              "      <td>1991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17284</th>\n",
              "      <td>-1.364289</td>\n",
              "      <td>0.389105</td>\n",
              "      <td>5418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17285</th>\n",
              "      <td>-3.028455</td>\n",
              "      <td>-2.639350</td>\n",
              "      <td>5275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17286</th>\n",
              "      <td>-1.531266</td>\n",
              "      <td>-4.170616</td>\n",
              "      <td>5055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17287</th>\n",
              "      <td>-0.517811</td>\n",
              "      <td>-4.688427</td>\n",
              "      <td>4818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17288</th>\n",
              "      <td>0.080706</td>\n",
              "      <td>-4.607721</td>\n",
              "      <td>4596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17289 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             V2        V1  Vazao\n",
              "0           NaN       NaN   2110\n",
              "1           NaN -0.995261   2089\n",
              "2     -0.871661 -1.866922   2050\n",
              "3     -0.084298 -1.951220   2010\n",
              "4      1.005946 -0.945274   1991\n",
              "...         ...       ...    ...\n",
              "17284 -1.364289  0.389105   5418\n",
              "17285 -3.028455 -2.639350   5275\n",
              "17286 -1.531266 -4.170616   5055\n",
              "17287 -0.517811 -4.688427   4818\n",
              "17288  0.080706 -4.607721   4596\n",
              "\n",
              "[17289 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data[['V2', 'V1', 'Vazao']]\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **MODELO 3**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bin1</th>\n",
              "      <th>Bin2</th>\n",
              "      <th>Bin3</th>\n",
              "      <th>V2</th>\n",
              "      <th>V1</th>\n",
              "      <th>Vazao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.871661</td>\n",
              "      <td>-1.866922</td>\n",
              "      <td>2050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.084298</td>\n",
              "      <td>-1.951220</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.005946</td>\n",
              "      <td>-0.945274</td>\n",
              "      <td>1991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.059247</td>\n",
              "      <td>-1.004520</td>\n",
              "      <td>1971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.004520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.507357</td>\n",
              "      <td>0.507357</td>\n",
              "      <td>1981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.507357</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.959112</td>\n",
              "      <td>0.959112</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.959112</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Bin1  Bin2  Bin3        V2        V1  Vazao\n",
              "2      0     1     0 -0.871661 -1.866922   2050\n",
              "3      0     1     0 -0.084298 -1.951220   2010\n",
              "4      0     1     0  1.005946 -0.945274   1991\n",
              "5      0     1     0 -0.059247 -1.004520   1971\n",
              "6      0     1     0  1.004520  0.000000   1971\n",
              "7      0     1     0  0.000000  0.000000   1971\n",
              "8      0     1     0  0.507357  0.507357   1981\n",
              "9      0     1     0 -0.507357  0.000000   1981\n",
              "10     0     1     0  0.959112  0.959112   2000\n",
              "11     0     1     0 -0.959112  0.000000   2000"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Adicionar colunas binárias ao DataFrame\n",
        "data_com_derivadas = calcular_derivadas(data)\n",
        "data_com_binarias = criar_colunas_binarias(data_com_derivadas).dropna()\n",
        "\n",
        "data_com_binarias[['Bin1', 'Bin2', 'Bin3', 'V2', 'V1', 'Vazao']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrames salvos com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Criar datasets com defasagem nas colunas binárias e 'Vazao'\n",
        "data_binarias_15 = criar_defasagens(data_com_binarias, ['Bin1', 'Bin2', 'Bin3', 'Vazao'], 15)\n",
        "data_binarias_30 = criar_defasagens(data_com_binarias, ['Bin1', 'Bin2', 'Bin3', 'Vazao'], 30)\n",
        "data_binarias_45 = criar_defasagens(data_com_binarias, ['Bin1', 'Bin2', 'Bin3', 'Vazao'], 45)\n",
        "\n",
        "# Pasta onde os arquivos serão salvos, dentro de 'data'\n",
        "output_folder = 'data/modelo_3'\n",
        "\n",
        "# Criar a pasta 'modelo_3' dentro de 'data' se ela não existir\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Caminhos para os arquivos Excel\n",
        "file_path_binarias_15 = os.path.join(output_folder, 'data_binarias_15_dias.xlsx')\n",
        "file_path_binarias_30 = os.path.join(output_folder, 'data_binarias_30_dias.xlsx')\n",
        "file_path_binarias_45 = os.path.join(output_folder, 'data_binarias_45_dias.xlsx')\n",
        "\n",
        "# Salvar os DataFrames em arquivos Excel\n",
        "data_binarias_15.to_excel(file_path_binarias_15, index=False)\n",
        "data_binarias_30.to_excel(file_path_binarias_30, index=False)\n",
        "data_binarias_45.to_excel(file_path_binarias_45, index=False)\n",
        "\n",
        "print(\"DataFrames salvos com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bin1</th>\n",
              "      <th>Bin2</th>\n",
              "      <th>Bin3</th>\n",
              "      <th>Vazao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17282</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17283</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17284</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17285</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17286</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17287 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Bin1  Bin2  Bin3  Vazao\n",
              "0         0     1     0   2050\n",
              "1         0     1     0   2010\n",
              "2         0     1     0   1991\n",
              "3         0     1     0   1971\n",
              "4         0     1     0   1971\n",
              "...     ...   ...   ...    ...\n",
              "17282     0     1     0   5418\n",
              "17283     0     1     0   5275\n",
              "17284     0     1     0   5055\n",
              "17285     0     1     0   4818\n",
              "17286     0     1     0   4596\n",
              "\n",
              "[17287 rows x 4 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "file_path = 'data/modelo_3/data_binarias_15_dias.xlsx'\n",
        "data3 = pd.read_excel(file_path,usecols=['Bin1','Bin2','Bin3','Vazao'])\n",
        "data3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TREINAMENTO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# Definir a seed\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Função para preparar dados para treinamento e teste\n",
        "def preparar_dados(data):\n",
        "    data = data.dropna()\n",
        "    X = data.iloc[:, :-12].values\n",
        "    y = data.iloc[:, -12:].values\n",
        "    return X, y\n",
        "\n",
        "# Função para construir o modelo MLP\n",
        "def criar_modelo(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(shape=(input_dim,)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Função para normalizar os dados\n",
        "def normalizar_dados(X_train, X_test):\n",
        "    scaler = StandardScaler()\n",
        "    X_train_normalized = scaler.fit_transform(X_train)\n",
        "    X_test_normalized = scaler.transform(X_test)\n",
        "    return X_train_normalized, X_test_normalized\n",
        "\n",
        "# Função para treinar e avaliar o modelo\n",
        "def treinar_e_avaliar(X_train, y_train, X_val, y_val, input_dim):\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "    history_list = []\n",
        "    num_epochs = 50\n",
        "    avg_train_loss_per_epoch = np.zeros(num_epochs)\n",
        "    avg_val_loss_per_epoch = np.zeros(num_epochs)\n",
        "    \n",
        "    for train_index, val_index in kfold.split(X_train):\n",
        "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "        X_train_fold, X_val_fold, _, _ = normalizar_dados(X_train_fold, X_val_fold, X_val_fold)\n",
        "\n",
        "        # Construir e treinar o modelo\n",
        "        model = criar_modelo(input_dim, y_train.shape[1])\n",
        "        \n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        \n",
        "        history = model.fit(X_train_fold, y_train_fold, epochs=num_epochs, batch_size=64, verbose=0,\n",
        "                            validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping])\n",
        "        \n",
        "        avg_train_loss_per_epoch += history.history['loss']\n",
        "        avg_val_loss_per_epoch += history.history['val_loss']       \n",
        "        history_list.append(history)\n",
        "    \n",
        "    # Calcular a média dos erros por época\n",
        "    avg_train_loss_per_epoch /= kfold.n_splits\n",
        "    avg_val_loss_per_epoch /= kfold.n_splits\n",
        "    \n",
        "    return model, history_list, avg_train_loss_per_epoch, avg_val_loss_per_epoch\n",
        "\n",
        "# Função para calcular o erro MAPE\n",
        "def avaliar_modelo(model, X_test, y_test, scaler_y, title):\n",
        "    X_test_scaled, _, _, _ = normalizar_dados(X_test, X_test, X_test)\n",
        "    \n",
        "    # Avaliar o modelo\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
        "    y_test_inv = scaler_y.inverse_transform(y_test)\n",
        "    \n",
        "    # Plotar previsões vs valores reais\n",
        "    criar_dataframe_e_plotar(y_pred, y_test, title=title)\n",
        "    \n",
        "    # Calcular MAPE\n",
        "    test_mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
        "    print(f'MAPE no conjunto de teste: {test_mape}')\n",
        "    return test_mape\n",
        "\n",
        "# Função para plotar o decaimento do erro\n",
        "def plot_avg_history(avg_train_loss, avg_val_loss, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(avg_train_loss, label='Treino')\n",
        "    plt.plot(avg_val_loss, label='Validação')\n",
        "    plt.title(f'Decaimento do Erro - {title}')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Erro')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Função para criar DataFrame e plotar gráfico\n",
        "def criar_dataframe_e_plotar(previsoes, valores_reais, n=15, title='Previsões vs. Valores Reais'):\n",
        "    df_comparacao = pd.DataFrame({\n",
        "        'Previsão': np.array(previsoes[:n]).flatten(),\n",
        "        'Reais': np.array(valores_reais[:n]).flatten()\n",
        "    })\n",
        "    \n",
        "    plt.figure(figsize=(9, 5))\n",
        "    plt.plot(df_comparacao['Previsão'], label='Previsão')\n",
        "    plt.plot(df_comparacao['Reais'], label='Reais')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Amostra')\n",
        "    plt.ylabel('Vazão')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    return df_comparacao\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TREINAMENTO - MODELO 1**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "# Mostrando as primeiras 5 linhas de X_15 e y_15\n",
        "\n",
        "print(\"X_15 (Entradas):\")\n",
        "print(pd.DataFrame(X_15).head())\n",
        "\n",
        "print(\"\\nY_15 (Saídas):\")\n",
        "print(pd.DataFrame(y_15).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## X_15 (Entradas):\n",
        "\n",
        "|     |   0    |   1    |   2    |   3    |   4    |   5    |   6    |   7    |   8    |   9    |   10   |   11   |   12   |   13   |   14   |\n",
        "|-----|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n",
        "| 0   | 2110.0 | 2089.0 | 2050.0 | 2010.0 | 1991.0 | 1971.0 | 1971.0 | 1971.0 | 1981.0 | 1981.0 | 2000.0 | 2000.0 | 2000.0 | 1981.0 | 1990.0 |\n",
        "| 1   | 2089.0 | 2050.0 | 2010.0 | 1991.0 | 1971.0 | 1971.0 | 1971.0 | 1981.0 | 1981.0 | 2000.0 | 2000.0 | 2000.0 | 1981.0 | 1990.0 | 1991.0 |\n",
        "| 2   | 2050.0 | 2010.0 | 1991.0 | 1971.0 | 1971.0 | 1971.0 | 1981.0 | 1981.0 | 2000.0 | 2000.0 | 2000.0 | 1981.0 | 1990.0 | 1991.0 | 1971.0 |\n",
        "| 3   | 2010.0 | 1991.0 | 1971.0 | 1971.0 | 1971.0 | 1981.0 | 1981.0 | 2000.0 | 2000.0 | 2000.0 | 1981.0 | 1990.0 | 1991.0 | 1971.0 | 1942.0 |\n",
        "| 4   | 1991.0 | 1971.0 | 1971.0 | 1971.0 | 1981.0 | 1981.0 | 2000.0 | 2000.0 | 2000.0 | 1981.0 | 1990.0 | 1991.0 | 1971.0 | 1942.0 | 1913.0 |\n",
        "\n",
        "## Y_15 (Saídas):\n",
        "\n",
        "|     |   0    |   1    |   2    |   3    |   4    |   5    |   6    |   7    |   8    |   9    |   10   |   11   |\n",
        "|-----|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n",
        "| 0   | 1991.0 | 1971.0 | 1942.0 | 1913.0 | 1875.0 | 1857.0 | 1829.0 | 1847.0 | 1875.0 | 1885.0 | 1904.0 | 1904.0 |\n",
        "| 1   | 1971.0 | 1942.0 | 1913.0 | 1875.0 | 1857.0 | 1829.0 | 1847.0 | 1875.0 | 1885.0 | 1904.0 | 1904.0 | 1913.0 |\n",
        "| 2   | 1942.0 | 1913.0 | 1875.0 | 1857.0 | 1829.0 | 1847.0 | 1875.0 | 1885.0 | 1904.0 | 1904.0 | 1913.0 | 1952.0 |\n",
        "| 3   | 1913.0 | 1875.0 | 1857.0 | 1829.0 | 1847.0 | 1875.0 | 1885.0 | 1904.0 | 1904.0 | 1913.0 | 1952.0 | 1981.0 |\n",
        "| 4   | 1875.0 | 1857.0 | 1829.0 | 1847.0 | 1875.0 | 1885.0 | 1904.0 | 1904.0 | 1913.0 | 1952.0 | 1981.0 | 2010.0 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar os dados do modelo 1\n",
        "modelo1_15 = pd.read_excel('data/modelo_1/data_15_dias.xlsx')\n",
        "modelo1_30 = pd.read_excel('data/modelo_1/data_30_dias.xlsx')\n",
        "modelo1_45 = pd.read_excel('data/modelo_1/data_45_dias.xlsx')\n",
        "# Definir a seed\n",
        "\n",
        "\n",
        "X_15, y_15 = preparar_dados(modelo1_15)\n",
        "X_30, y_30 = preparar_dados(modelo1_30)\n",
        "X_45, y_45 = preparar_dados(modelo1_45)\n",
        "\n",
        "defasagens = [15, 30, 45]\n",
        "datasets = [(X_15, y_15), (X_30, y_30), (X_45, y_45)]\n",
        "\n",
        "mape_results_1 = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Função custo(MSE) e Predições"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "for defasagem, (X, y) in zip(defasagens, datasets):\n",
        "    print(f\"\\nTreinando e avaliando com defasagem de {defasagem} dias...\")\n",
        "    \n",
        "    # Separar os dados em treino, validação e teste\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed)\n",
        "    \n",
        "    # Treinar e avaliar o modelo\n",
        "    model, history, avg_train_loss, avg_val_loss = treinar_e_avaliar(X_train, y_train, X_val, y_val, X_train.shape[1])\n",
        "    plot_avg_history(avg_train_loss, avg_val_loss, f'Modelo 1 - Defasagem de {defasagem}')\n",
        "    \n",
        "    # Normalizar o target para inversão posterior\n",
        "    scaler_y = StandardScaler()\n",
        "    scaler_y.fit(y_train)\n",
        "    \n",
        "    # Avaliar o modelo no conjunto de teste\n",
        "    mape = avaliar_modelo(model, X_test, y_test, scaler_y, f'Previsões vs. Valores Reais - Modelo 1 - Defasagem de {defasagem}')\n",
        "    mape_results_1.append(mape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MAPE médio do Modelo 1: 16.26%\n"
          ]
        }
      ],
      "source": [
        "# Calcular a média dos MAPE do modelo 1\n",
        "avg_mape_modelo1 = np.mean(mape_results_1)\n",
        "print(f'\\nMAPE médio do Modelo 1: {avg_mape_modelo1*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TREINAMENTO - MODELO 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Configurando a seed para reprodutibilidade\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "def preparar_dados(data):\n",
        "    data = data.dropna()\n",
        "    X = data.iloc[:, :-12].values\n",
        "    y = data.iloc[:, -12:].values\n",
        "    return X, y\n",
        "\n",
        "def normalizar_dados(X_train, X_test):\n",
        "    scaler = StandardScaler()\n",
        "    X_train_normalized = scaler.fit_transform(X_train)\n",
        "    X_test_normalized = scaler.transform(X_test)\n",
        "    return X_train_normalized, X_test_normalized\n",
        "\n",
        "def criar_modelo(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(shape=(input_dim,)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def treinar_modelo(X_train_val, y_train_val):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "    history_list = []\n",
        "\n",
        "    for train_index, val_index in kf.split(X_train_val):\n",
        "        X_train_fold, X_val_fold = X_train_val[train_index], X_train_val[val_index]\n",
        "        y_train_fold, y_val_fold = y_train_val[train_index], y_train_val[val_index]\n",
        "\n",
        "        model = criar_modelo(X_train_fold.shape[1], y_train_fold.shape[1])\n",
        "        history = model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=64, verbose=0, validation_data=(X_val_fold, y_val_fold))\n",
        "        history_list.append(history)\n",
        "\n",
        "    return model, history_list\n",
        "\n",
        "def avaliar_modelo(model, X_test, y_test,title):\n",
        "    y_pred = model.predict(X_test)\n",
        "    plot_previsoes_vs_reais(y_pred, y_test, title=title)\n",
        "    test_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "    print(f'MAPE no conjunto de teste: {test_mape}')\n",
        "    return test_mape\n",
        "\n",
        "def plot_avg_history(history_list,title):\n",
        "    epochs = range(1, len(history_list[0].history['loss']) + 1)\n",
        "    \n",
        "    # Inicializando listas para armazenar as perdas médias por época\n",
        "    avg_train_loss = np.zeros(len(epochs))\n",
        "    avg_val_loss = np.zeros(len(epochs))\n",
        "\n",
        "    for history in history_list:\n",
        "        avg_train_loss += np.array(history.history['loss'])\n",
        "        avg_val_loss += np.array(history.history['val_loss'])\n",
        "    \n",
        "    # Calculando as médias\n",
        "    avg_train_loss /= len(history_list)\n",
        "    avg_val_loss /= len(history_list)\n",
        "\n",
        "    # Plotando as perdas médias\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, avg_train_loss, 'b', label='Perda de Treinamento Média')\n",
        "    plt.plot(epochs, avg_val_loss, 'r', label='Perda de Validação Média')\n",
        "    plt.title(f'Decaimento do Erro - {title}')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Erro Médio Quadrático')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "        \n",
        "def plot_previsoes_vs_reais(y_pred, y_test, n=15, title='Previsões vs. Valores Reais'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test[:n].flatten(), label='Valores Reais')\n",
        "    plt.plot(y_pred[:n].flatten(), label='Previsões')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Amostra')\n",
        "    plt.ylabel('Vazão')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Carregando os dados\n",
        "modelo2_15 = pd.read_excel('data/modelo_2/data_derivadas_15_dias.xlsx')\n",
        "modelo2_30 = pd.read_excel('data/modelo_2/data_derivadas_30_dias.xlsx')\n",
        "modelo2_45 = pd.read_excel('data/modelo_2/data_derivadas_45_dias.xlsx')\n",
        "\n",
        "# Preparando os dados para cada conjunto de dados\n",
        "X_15, y_15 = preparar_dados(modelo2_15)\n",
        "X_30, y_30 = preparar_dados(modelo2_30)\n",
        "X_45, y_45 = preparar_dados(modelo2_45)\n",
        "\n",
        "defasagens = [15, 30, 45]\n",
        "datasets = [(X_15, y_15), (X_30, y_30), (X_45, y_45)]\n",
        "\n",
        "mape_results_1 = []\n",
        "for defasagem, (X, y) in zip(defasagens, datasets):\n",
        "    print(f\"\\nTreinando e avaliando com defasagem de {defasagem} dias...\")\n",
        "\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    X_train_val, X_test = normalizar_dados(X_train_val, X_test)\n",
        "    \n",
        "    model, history_list = treinar_modelo(X_train_val, y_train_val)\n",
        "    plot_avg_history(history_list,f'Modelo 1 - Defasagem de {defasagem}')\n",
        "\n",
        "    mape = avaliar_modelo(model, X_test, y_test,f'Previsões vs. Valores Reais - Modelo 1 - Defasagem de {defasagem}')\n",
        "    mape_results_1.append(mape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Treinando e avaliando com defasagem de 15 dias...\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step\n",
            "MAPE no conjunto de teste: 8.14%\n",
            "\n",
            "Treinando e avaliando com defasagem de 30 dias...\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step\n",
            "MAPE no conjunto de teste: 8.34%\n",
            "\n",
            "Treinando e avaliando com defasagem de 45 dias...\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step\n",
            "MAPE no conjunto de teste: 8.94%\n",
            "\n",
            "MAPE médio do Modelo 1: 8.48%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "def preparar_dados(data):\n",
        "    data = data.dropna()\n",
        "    X = data.iloc[:, :-12].values\n",
        "    y = data.iloc[:, -12:].values\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def criar_modelo(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(shape=(input_dim,)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "\n",
        "modelo2_15 = pd.read_excel('data/modelo_2/data_derivadas_15_dias.xlsx')\n",
        "modelo2_30 = pd.read_excel('data/modelo_2/data_derivadas_30_dias.xlsx')\n",
        "modelo2_45 = pd.read_excel('data/modelo_2/data_derivadas_45_dias.xlsx')\n",
        "\n",
        "X_15, y_15 = preparar_dados(modelo2_15)\n",
        "X_30, y_30 = preparar_dados(modelo2_30)\n",
        "X_45, y_45 = preparar_dados(modelo2_45)\n",
        "\n",
        "defasagens = [15, 30, 45]\n",
        "datasets = [(X_15, y_15), (X_30, y_30), (X_45, y_45)]\n",
        "\n",
        "mape_results_1 = []\n",
        "\n",
        "for defasagem, (X, y) in zip(defasagens, datasets):\n",
        "    print(f\"\\nTreinando e avaliando com defasagem de {defasagem} dias...\")\n",
        "    # Divisão dos dados em treino e teste\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    \n",
        "    #normalização dos dados\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    \n",
        "    model = criar_modelo(X_train.shape[1], y_train.shape[1])\n",
        "    history = model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0, validation_split=0.25)\n",
        "    \n",
        "    # Avaliar o modelo com mape\n",
        "    y_pred = model.predict(X_test)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "    print(f'MAPE no conjunto de teste: {mape*100:.2f}%')\n",
        "    mape_results_1.append(mape)\n",
        "   \n",
        "avg_mape_modelo1 = np.mean(mape_results_1)\n",
        "print(f'\\nMAPE médio do Modelo 1: {avg_mape_modelo1*100:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
